{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    " \n",
    "# To help construct our Chat Messages\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    " \n",
    "# We will be using ChatGPT model (gpt-3.5-turbo)\n",
    "from langchain.chat_models import ChatOpenAI\n",
    " \n",
    "# To parse outputs and get structured data back\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    " \n",
    "# Enter your API Key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "text = \"\"\"\n",
    "Adoption of 3D printing has reached critical mass as those who have yet \n",
    "to integrate additive manufacturing somewhere in their supply chain are \n",
    "now part of an ever-shrinking minority. Where 3D printing was only suitable \n",
    "for prototyping and one-off manufacturing in the early stages, it is now \n",
    "rapidly transforming into a production technology.\n",
    " \n",
    "Most of the current demand for 3D printing is industrial in nature. \n",
    "Acumen Research and Consulting forecasts the global 3D printing market \n",
    "to reach $41 billion by 2026.\n",
    " \n",
    "As it evolves, 3D printing technology is destined to transform almost \n",
    "every major industry and change the way we live, work, and play in the future.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"question\": string  // A multiple choice question generated from input text snippet.\n",
      "\t\"option_1\": string  // First option for the multiple choice question. Use this format: 'a) option'\n",
      "\t\"option_2\": string  // Second option for the multiple choice question. Use this format: 'b) option'\n",
      "\t\"option_3\": string  // Third option for the multiple choice question. Use this format: 'c) option'\n",
      "\t\"option_4\": string  // Fourth option for the multiple choice question. Use this format: 'd) option'\n",
      "\t\"answer\": string  // Correct answer for the question. Use this format: 'd) option' or 'b) option', etc.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# The schema I want out\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"question\", description=\"A multiple choice question generated from input text snippet.\"),\n",
    "    ResponseSchema(name=\"option_1\", description=\"First option for the multiple choice question. Use this format: 'a) option'\"),\n",
    "    ResponseSchema(name=\"option_2\", description=\"Second option for the multiple choice question. Use this format: 'b) option'\"),\n",
    "    ResponseSchema(name=\"option_3\", description=\"Third option for the multiple choice question. Use this format: 'c) option'\"),\n",
    "    ResponseSchema(name=\"option_4\", description=\"Fourth option for the multiple choice question. Use this format: 'd) option'\"),\n",
    "    ResponseSchema(name=\"answer\", description=\"Correct answer for the question. Use this format: 'd) option' or 'b) option', etc.\")\n",
    "]\n",
    " \n",
    "# The parser that will look for the LLM output in my schema and return it back to me\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    " \n",
    "# The format instructions that LangChain makes. Let's look at them\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    " \n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\koush\\anaconda3\\envs\\myFastapi\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ChatGPT object\n",
    "# chat_model = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')\n",
    "chat_model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatPromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# The prompt template that brings it all together\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mChatPromptTemplate\u001b[49m(\n\u001b[0;32m      3\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      4\u001b[0m         HumanMessagePromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mGiven a text input, generate multiple choice questions \u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m        from it along with the correct answer. \u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{format_instructions}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{user_prompt}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m)  \n\u001b[0;32m      7\u001b[0m     ],\n\u001b[0;32m      8\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      9\u001b[0m     partial_variables\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_instructions\u001b[39m\u001b[38;5;124m\"\u001b[39m: format_instructions}\n\u001b[0;32m     10\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ChatPromptTemplate' is not defined"
     ]
    }
   ],
   "source": [
    "# The prompt template that brings it all together\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\"\"\"Given a text input, generate multiple choice questions \n",
    "        from it along with the correct answer. \n",
    "        \\n{format_instructions}\\n{user_prompt}\"\"\")  \n",
    "    ],\n",
    "    input_variables=[\"user_prompt\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\koush\\anaconda3\\envs\\myFastapi\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:115: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"question\": \"What is the forecasted value of the global 3D printing market by 2026?\",\n",
      "\t\"option_1\": \"a) $41 million\",\n",
      "\t\"option_2\": \"b) $41 billion\",\n",
      "\t\"option_3\": \"c) $41 trillion\",\n",
      "\t\"option_4\": \"d) $41 quadrillion\",\n",
      "\t\"answer\": \"b) $41 billion\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "user_query = prompt.format_prompt(user_prompt = text)\n",
    "user_query_output = chat_model(user_query.to_messages())\n",
    " \n",
    "print(user_query_output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE=\"\"\"\n",
    "Text:{text}\n",
    "You are an expert MCQ maker. Given the above text, it is your job to \\\n",
    "create a quiz  of {number} multiple choice questions for {subject} students in {tone} tone. \n",
    "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
    "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "{RESPONSE_JSON}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_generation_prompt=PromptTemplate(\n",
    "    input_variables=[\"text\",\"number\",\"subject\",\"tone\",\"RESPONSE_JSON\"],\n",
    "    template=TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LLMChain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m quiz_chain\u001b[38;5;241m=\u001b[39m\u001b[43mLLMChain\u001b[49m(llm\u001b[38;5;241m=\u001b[39mllm,prompt\u001b[38;5;241m=\u001b[39mquiz_generation_prompt,output_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquiz\u001b[39m\u001b[38;5;124m\"\u001b[39m,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LLMChain' is not defined"
     ]
    }
   ],
   "source": [
    "quiz_chain=LLMChain(llm=llm,prompt=quiz_generation_prompt,output_key=\"quiz\",verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE2=\"\"\"\n",
    "You are an expert english grammarian and writer. Given a Multiple Choice Quiz for {subject} students.\\\n",
    "You need to evaluate the complexity of the question and give a complete analysis of the quiz. Only use at max 50 words for complexity analysis. \n",
    "if the quiz is not at per with the cognitive and analytical abilities of the students,\\\n",
    "update the quiz questions which needs to be changed and change the tone such that it perfectly fits the student abilities\n",
    "Quiz_MCQs:\n",
    "{quiz}\n",
    "\n",
    "Check from an expert English Writer of the above quiz:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_evaluation_prompt=PromptTemplate(input_variables=[\"subject\", \"quiz\"], template=TEMPLATE2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chain=LLMChain(llm=llm,prompt=quiz_evaluation_prompt,output_key=\"review\",verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluate_chain=SequentialChain(chains=[quiz_chain, review_chain], input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"RESPONSE_JSON\"],output_variables=[\"quiz\", \"review\"], verbose=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"D:/Professional/Project/FinalYr/Eduverse_backend_FASTapi/documents/data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH,\"r\") as file:\n",
    "    TEXT=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent progress in generative models have resulted in models that can produce realistic text, images and video that can potentially revolutionize the way humans work, create content and interact with machines. The workshop on Generative AI at AIMLSystems will focus on the entire life-cycle of building and deploying such Generative AI systems, including data collection and processing, developing systems and requisite infrastructure, applications it enables, and the ethics associated with such technology covering concerns related to fairness, transparency and accountability. We invite original, unpublished work on Artificial Intelligence with a focus on generative AI and their use cases. Specifically, the topics of interest include but are not limited to:\n",
      "\n",
      "Systems, architecture and infrastructure for Generative AI\n",
      "Machine learning and Modeling using LLMs and Diffusion models\n",
      "Large Language Models and its applications\n",
      "Multi-modal Generative AI and its applications\n",
      "Gen AI based Plugins and agents\n",
      "Deployment of Generative AI solutions\n",
      "Evaluation of Language and Diffusion based models\n",
      "Responsible use of Gen AI\n"
     ]
    }
   ],
   "source": [
    "print(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialChain(verbose=True, chains=[LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['RESPONSE_JSON', 'number', 'subject', 'text', 'tone'], template='\\nText:{text}\\nYou are an expert MCQ maker. Given the above text, it is your job to create a quiz  of {number} multiple choice questions for {subject} students in {tone} tone. \\nMake sure the questions are not repeated and check all the questions to be conforming the text as well.\\nMake sure to format your response like  RESPONSE_JSON below  and use it as a guide. Ensure to make {number} MCQs\\n### RESPONSE_JSON\\n{RESPONSE_JSON}\\n\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000019AC0B55E40>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000019AC0B90610>, openai_api_key='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', openai_proxy=''), output_key='quiz'), LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['quiz', 'subject'], template='\\nYou are an expert english grammarian and writer. Given a Multiple Choice Quiz for {subject} students.You need to evaluate the complexity of the question and give a complete analysis of the quiz. Only use at max 50 words for complexity analysis. \\nif the quiz is not at per with the cognitive and analytical abilities of the students,update the quiz questions which needs to be changed and change the tone such that it perfectly fits the student abilities\\nQuiz_MCQs:\\n{quiz}\\n\\nCheck from an expert English Writer of the above quiz:\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000019AC0B55E40>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000019AC0B90610>, openai_api_key='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', openai_proxy=''), output_key='review')], input_variables=['text', 'number', 'subject', 'tone', 'RESPONSE_JSON'], output_variables=['quiz', 'review'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_evaluate_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with  get_openai_callback() as cb:\n",
    "    response=generate_evaluate_chain(\n",
    "        {\n",
    "\n",
    "        \"text\": TEXT,\n",
    "        \"number\": NUMBER,\n",
    "        \"subject\": SUBJECT,\n",
    "        \"tone\": TONE,\n",
    "        \"RESPONSE_JSON\": json.dumps(RESPONSE_JSON)\n",
    "\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myFastapi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
